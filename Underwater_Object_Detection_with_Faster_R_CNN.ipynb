{
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3174116,
          "sourceType": "datasetVersion",
          "datasetId": 1929044
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Underwater Object Detection with Faster R-CNN",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudrani-rane/Underwater-Object-Detection-And-Classification-Using-YOLOv8/blob/main/Underwater_Object_Detection_with_Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "slavkoprytula_aquarium_data_cots_path = kagglehub.dataset_download('slavkoprytula/aquarium-data-cots')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "zklfhZ-rkz_k"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Underwater Object Detection with Faster R-CNN\n",
        "\n",
        "This notebook is a step-by-step tutorial on how to train a Faster R-CNN model for object detection for underwater images. The dataset used in this notebook is the [Underwater Image Dataset](https://www.kaggle.com/datasets/slavkoprytula/aquarium-data-cots)."
      ],
      "metadata": {
        "id": "UJFmayzOkz_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "Object detection is a computer vision task that involves both localizing objects in images and classifying them. Faster R-CNN is a popular deep learning model for object detection that is an evolution of the R-CNN and Fast R-CNN models. Faster R-CNN is composed of two modules: a region proposal network (RPN) that generates region proposals and a network that uses these proposals to detect objects. You can learn more about this network [here](https://arxiv.org/abs/1506.01497). The architecture used in this notebook is based on [this paper](https://www.researchgate.net/publication/368708716_Underwater_Object_Detection_Method_Based_on_Improved_Faster_RCNN)."
      ],
      "metadata": {
        "id": "WCawg8vzkz_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will use some techniques to improve the performance of the model, such as data augmentation, transfer learning, and hyperparameter tuning, OHEM, GIOU loss, and Soft-NMS.\n",
        "\n",
        "- **Data Augmentation**: Data augmentation is a technique used to artificially increase the size of the training dataset by applying transformations to the images. This technique can help the model generalize better to new data and improve its performance.\n",
        "- **Transfer Learning**: Transfer learning is a technique that allows you to use a pre-trained model as a starting point for training a new model on a different task. This can help you achieve better performance with less data and computational resources.\n",
        "- **Hyperparameter Tuning**: Hyperparameter tuning is the process of finding the best set of hyperparameters for a machine learning model. This can help you improve the performance of the model and reduce the risk of overfitting.\n",
        "- **OHEM**: Online Hard Example Mining (OHEM) is a technique used to focus the training process on the most challenging examples in the dataset. This can help the model learn from its mistakes and improve its performance.\n",
        "- **GIOU Loss**: Generalized Intersection over Union (GIOU) is a loss function used to measure the similarity between two bounding boxes. This loss function can help the model learn to predict more accurate bounding boxes.\n",
        "- **Soft-NMS**: Soft Non-Maximum Suppression (Soft-NMS) is a technique used to suppress overlapping bounding boxes by reducing the confidence scores of nearby boxes. This can help the model produce more accurate detections."
      ],
      "metadata": {
        "id": "mGxqegpZkz_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import transforms\n",
        "import torchvision.ops as ops\n",
        "from torch.utils.data import distributed, RandomSampler, SequentialSampler\n",
        "import random\n",
        "import cv2"
      ],
      "metadata": {
        "id": "CKrTS8ajkz_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "PZ-VUbvMkz_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA (Exploratory Data Analysis)\n",
        "\n",
        "we will start by loading the dataset and exploring its contents. The dataset contains images of underwater scenes with annotations for the objects present in the images. We will load the images and annotations, visualize some examples, and analyze the distribution of object classes in the dataset."
      ],
      "metadata": {
        "id": "bpmlIIvekz_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"../data/aquarium_pretrain/train\"\n",
        "VAL_PATH = \"../data/aquarium_pretrain/valid\"\n",
        "TEST_PATH = \"../data/aquarium_pretrain/test\"\n",
        "\n",
        "TRAIN_LABELS = TRAIN_PATH + \"/labels\"\n",
        "VAL_LABELS = VAL_PATH + \"/labels\"\n",
        "TEST_LABELS = TEST_PATH + \"/labels\"\n",
        "\n",
        "TRAIN_IMAGES = TRAIN_PATH + \"/images\"\n",
        "VAL_IMAGES = VAL_PATH + \"/images\"\n",
        "TEST_IMAGES = TEST_PATH + \"/images\"\n",
        "\n",
        "data = \"../data/aquarium_pretrain/data.yaml\""
      ],
      "metadata": {
        "id": "2qatCT4Qkz_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"fish\", \"jellyfish\", \"penguin\", \"puffin\", \"shark\", \"starfish\", \"stingray\"]\n",
        "Idx2Label = {idx: label for idx, label in enumerate(classes)}\n",
        "Label2Index = {label: idx for idx, label in Idx2Label.items()}\n",
        "print(\"Index to Label Mapping:\", Idx2Label)\n",
        "print(\"Label to Index Mapping:\", Label2Index)"
      ],
      "metadata": {
        "id": "2vvXnNHEkz_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_num_images(data, title):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    ax.bar(data.keys(), data.values())\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def get_num_images(path):\n",
        "    return len(os.listdir(path))\n",
        "\n",
        "plot_num_images({ \"train\": get_num_images(TRAIN_IMAGES), \"val\": get_num_images(VAL_IMAGES), \"test\": get_num_images(TEST_IMAGES) }, \"Number of Images in Each Dataset\")"
      ],
      "metadata": {
        "id": "illIlCHLkz_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_objects_per_images(image_dir, label_dir, object_count, total_object_count):\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    for image_file in tqdm.tqdm(image_files):\n",
        "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
        "        f = open(label_path, \"r\")\n",
        "        lines = f.readlines()\n",
        "        total_object_count.append(len(lines))\n",
        "        for line in lines:\n",
        "            object_count[Idx2Label[int(line.split()[0])]] = object_count.get(Idx2Label[int(line.split()[0])], 0) + 1\n",
        "        f.close()\n",
        "\n",
        "train_object_count = {}\n",
        "val_object_count = {}\n",
        "test_object_count = {}\n",
        "\n",
        "total_object_per_train_count = []\n",
        "total_object_per_val_count = []\n",
        "total_object_per_test_count = []\n",
        "\n",
        "count_objects_per_images(TRAIN_IMAGES, TRAIN_LABELS, train_object_count, total_object_per_train_count)\n",
        "count_objects_per_images(VAL_IMAGES, VAL_LABELS, val_object_count, total_object_per_val_count)\n",
        "count_objects_per_images(TEST_IMAGES, TEST_LABELS, test_object_count, total_object_per_test_count)"
      ],
      "metadata": {
        "id": "LF5cQ6P1kz_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(train_object_count)\n",
        "print()\n",
        "pp.pprint(val_object_count)\n",
        "print()\n",
        "pp.pprint(test_object_count)"
      ],
      "metadata": {
        "id": "z_N7y1sGkz_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_object_count(object_count, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(object_count.keys(), object_count.values())\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_object_count(train_object_count, \"Train Object Count\")\n",
        "plot_object_count(val_object_count, \"Validation Object Count\")\n",
        "plot_object_count(test_object_count, \"Test Object Count\")"
      ],
      "metadata": {
        "id": "vjY0kqmVkz__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_object_count = {label: train_object_count.get(label, 0) + val_object_count.get(label, 0) + test_object_count.get(label, 0) for label in classes}\n",
        "plot_object_count(total_object_count, \"Total Object Count\")"
      ],
      "metadata": {
        "id": "1GzFgBc2k0AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of classes in the datasets are shown, as can be seen fishes are the most common class in the dataset, followed by jellyfish and penguins. This imbalance in the dataset can affect the performance of the model, so we will use techniques like data augmentation and OHEM to address this issue."
      ],
      "metadata": {
        "id": "e0idaalKk0AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_total_object_per_image(total_object_per_image, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(total_object_per_image, bins=20)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_total_object_per_image(total_object_per_train_count, \"Total Object Per Train Image\")\n",
        "plot_total_object_per_image(total_object_per_val_count, \"Total Object Per Validation Image\")\n",
        "plot_total_object_per_image(total_object_per_test_count, \"Total Object Per Test Image\")"
      ],
      "metadata": {
        "id": "rfjm-Bg2k0AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of objects per image in all the datasets is shown, as can be seen most of the images contain 1-2 objects. All datasets have a similar distribution of objects per image, so we can use the same techniques for all datasets."
      ],
      "metadata": {
        "id": "Mui4xt_sk0AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bbox_sizes(image_dir, label_dir, sizes):\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    for image_file in tqdm.tqdm(image_files):\n",
        "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
        "        f = open(label_path, \"r\")\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            sizes.append(float(line.split()[3]) * float(line.split()[4]))\n",
        "        f.close()\n",
        "\n",
        "train_sizes = []\n",
        "val_sizes = []\n",
        "test_sizes = []\n",
        "\n",
        "get_bbox_sizes(TRAIN_IMAGES, TRAIN_LABELS, train_sizes)\n",
        "get_bbox_sizes(VAL_IMAGES, VAL_LABELS, val_sizes)\n",
        "get_bbox_sizes(TEST_IMAGES, TEST_LABELS, test_sizes)"
      ],
      "metadata": {
        "id": "s35O1oRtk0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_bbox_sizes(sizes, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(sizes, bins=20)\n",
        "    plt.title(title)\n",
        "    plt.xlim(0, .2)\n",
        "    plt.show()\n",
        "\n",
        "plot_bbox_sizes(train_sizes, \"Train Bounding Box Sizes\")\n",
        "plot_bbox_sizes(val_sizes, \"Validation Bounding Box Sizes\")\n",
        "plot_bbox_sizes(test_sizes, \"Test Bounding Box Sizes\")"
      ],
      "metadata": {
        "id": "JwWPV817k0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of bounding box sizes in the datasets is shown. It can help us understand the scale of the objects in the images and choose an appropriate anchor size for the model. It can also be really helpful for our anchor generation."
      ],
      "metadata": {
        "id": "GYMCC1ewk0AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_bbox_sizes(train_sizes, val_sizes, test_sizes, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(train_sizes, bins=20, alpha=0.5, label=\"Train\")\n",
        "    plt.hist(val_sizes, bins=20, alpha=0.5, label=\"Validation\")\n",
        "    plt.hist(test_sizes, bins=20, alpha=0.5, label=\"Test\")\n",
        "    plt.title(title)\n",
        "    plt.xlim(0, .2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_all_bbox_sizes(train_sizes, val_sizes, test_sizes, \"All Bounding Box Sizes\")"
      ],
      "metadata": {
        "id": "1U4R0PTlk0AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_width_height(image_dir, label_dir, width_height):\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    for image_file in tqdm.tqdm(image_files):\n",
        "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
        "        f = open(label_path, \"r\")\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            width_height.append((float(line.split()[3]), float(line.split()[4])))\n",
        "        f.close()\n",
        "\n",
        "train_width_height = []\n",
        "val_width_height = []\n",
        "test_width_height = []\n",
        "\n",
        "count_width_height(TRAIN_IMAGES, TRAIN_LABELS, train_width_height)\n",
        "count_width_height(VAL_IMAGES, VAL_LABELS, val_width_height)\n",
        "count_width_height(TEST_IMAGES, TEST_LABELS, test_width_height)"
      ],
      "metadata": {
        "id": "Zb-AZm2Gk0AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_width_height(width_height, title):\n",
        "    width, height = zip(*width_height)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(width, bins=20, alpha=0.5, label=\"Width\")\n",
        "    plt.hist(height, bins=20, alpha=0.5, label=\"Height\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_width_height(train_width_height, \"Train Width and Height\")\n",
        "plot_width_height(val_width_height, \"Validation Width and Height\")\n",
        "plot_width_height(test_width_height, \"Test Width and Height\")"
      ],
      "metadata": {
        "id": "FqevToSFk0AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_width_height = train_width_height + val_width_height + test_width_height\n",
        "plot_width_height(total_width_height, \"Width vs Height\")"
      ],
      "metadata": {
        "id": "eWQQn0u8k0AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_width_height_scatter(width_height, title):\n",
        "    width, height = zip(*width_height)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(width, height)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_width_height_scatter(train_width_height, \"Train Width vs Height\")\n",
        "plot_width_height_scatter(val_width_height, \"Validation Width vs Height\")\n",
        "plot_width_height_scatter(test_width_height, \"Test Width vs Height\")"
      ],
      "metadata": {
        "id": "yFlND5l3k0AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_width_height_scatter(train_width_height, val_width_height, test_width_height, title):\n",
        "    train_width, train_height = zip(*train_width_height)\n",
        "    val_width, val_height = zip(*val_width_height)\n",
        "    test_width, test_height = zip(*test_width_height)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(train_width, train_height, alpha=0.5, label=\"Train\")\n",
        "    plt.scatter(val_width, val_height, alpha=0.5, label=\"Validation\")\n",
        "    plt.scatter(test_width, test_height, alpha=0.5, label=\"Test\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_all_width_height_scatter(train_width_height, val_width_height, test_width_height, \"All Width vs Height\")"
      ],
      "metadata": {
        "id": "vwizIxJXk0AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_aspect_ratios(train_width_height, val_width_height, test_width_height, title):\n",
        "    EPSILON = 1e-6\n",
        "\n",
        "    train_aspect_ratio = [w / h if h != 0 else w / (h + EPSILON) for w, h in train_width_height]\n",
        "    val_aspect_ratio = [w / h if h != 0 else w / (h + EPSILON) for w, h in val_width_height]\n",
        "    test_aspect_ratio = [w / h if h != 0 else w / (h + EPSILON) for w, h in test_width_height]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(train_aspect_ratio, alpha=0.5, label=\"Train\", bins=50)\n",
        "    plt.hist(val_aspect_ratio, alpha=0.5, label=\"Validation\", bins=50)\n",
        "    plt.hist(test_aspect_ratio, alpha=0.5, label=\"Test\", bins=50)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_all_aspect_ratios(train_width_height, val_width_height, test_width_height, \"All Aspect Ratios\")"
      ],
      "metadata": {
        "id": "ZCpKUhypk0AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The width and height of bounding boxes can also help us understand the aspect ratio of the objects in the images and choose an appropriate anchor aspect ratio for the model."
      ],
      "metadata": {
        "id": "8lxV8xnok0AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_image_with_annotation_bounding_boxes(image_dir, label_dir):\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "\n",
        "    sample_image_files = random.sample(image_files, 12)\n",
        "\n",
        "    fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
        "\n",
        "    for i, image_file in enumerate(sample_image_files):\n",
        "        row = i // 3\n",
        "        col = i % 3\n",
        "\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
        "        f = open(label_path, \"r\")\n",
        "\n",
        "        for label in f:\n",
        "            class_id, x_center, y_center, width, height = map(float, label.split())\n",
        "            h, w, _ = image.shape\n",
        "            x_min = int((x_center - width / 2) * w)\n",
        "            y_min = int((y_center - height / 2) * h)\n",
        "            x_max = int((x_center + width / 2) * w)\n",
        "            y_max = int((y_center + height / 2) * h)\n",
        "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "            cv2.putText(\n",
        "                image,\n",
        "                Idx2Label[int(class_id)],\n",
        "                (x_min, y_min),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                fontScale=1,\n",
        "                color=(255, 255, 255),\n",
        "                thickness=2,\n",
        "            )\n",
        "\n",
        "        axs[row, col].imshow(image)\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_image_with_annotation_bounding_boxes(TRAIN_IMAGES, TRAIN_LABELS)"
      ],
      "metadata": {
        "id": "byqB6gMnk0AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "In this step, we will preprocess the dataset by converting the annotations to the format required by the Faster R-CNN model."
      ],
      "metadata": {
        "id": "yGBBC3FEk0AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(im, img_size=640, square=False):\n",
        "    if square:\n",
        "        im = cv2.resize(im, (img_size, img_size))\n",
        "    else:\n",
        "        h0, w0 = im.shape[:2]\n",
        "        r = img_size / max(h0, w0)\n",
        "        if r != 1:\n",
        "            im = cv2.resize(im, (int(w0 * r), int(h0 * r)))\n",
        "    return im"
      ],
      "metadata": {
        "id": "TEgokWsgk0AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DESTINATION_PATH = \"./aquarium_pretrain\"\n",
        "\n",
        "TRAIN_DESTINATION = DESTINATION_PATH + \"/train\"\n",
        "VAL_DESTINATION = DESTINATION_PATH + \"/valid\"\n",
        "TEST_DESTINATION = DESTINATION_PATH + \"/test\""
      ],
      "metadata": {
        "id": "PW8COOPwk0AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_dataset_and_update_labels(image_dir, label_dir, image_dest, label_dest, img_size=640, square=False):\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        image = resize(cv2.imread(image_path), img_size, square)\n",
        "        cv2.imwrite(os.path.join(image_dest, image_file), image)\n",
        "\n",
        "        label_path = os.path.join(label_dir, image_file[:-4] + \".txt\")\n",
        "        f = open(label_path, \"r\")\n",
        "        lines = f.readlines()\n",
        "        f.close()\n",
        "\n",
        "        f = open(os.path.join(label_dest, image_file[:-4] + \".txt\"), \"w\")\n",
        "        for line in lines:\n",
        "            class_id, x_center, y_center, width, height = map(float, line.split())\n",
        "            x_center *= img_size\n",
        "            y_center *= img_size\n",
        "            width *= img_size\n",
        "            height *= img_size\n",
        "            f.write(\"{} {} {} {} {}\\n\".format(int(class_id), x_center, y_center, width, height))\n",
        "        f.close()"
      ],
      "metadata": {
        "id": "vA8WQZvjk0AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_dataset_and_update_labels(TRAIN_IMAGES, TRAIN_LABELS, TRAIN_DESTINATION + \"/images\", TRAIN_DESTINATION + \"/labels\")\n",
        "resize_dataset_and_update_labels(VAL_IMAGES, VAL_LABELS, VAL_DESTINATION + \"/images\", VAL_DESTINATION + \"/labels\")\n",
        "resize_dataset_and_update_labels(TEST_IMAGES, TEST_LABELS, TEST_DESTINATION + \"/images\", TEST_DESTINATION + \"/labels\")"
      ],
      "metadata": {
        "id": "VHQV4hvMk0AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation\n",
        "\n",
        "For this dataset (images with regions of interest), we will use the following data augmentation techniques:\n",
        "\n",
        "- **RandomHorizontalFlip**: Randomly flips the image horizontally, here we should be careful to update the labels of objects in the image as well.\n",
        "- **RandomVerticalFlip**: Randomly flips the image vertically, here we should be careful to update the labels of objects in the image as well.\n",
        "- **RandomRotation**: Randomly rotates the image by a given angle, here we should be careful to update the labels of objects in the image as well.\n",
        "- **RandomResizedCrop**: Randomly crops the image to a given size and resizes it to the original size, here we should be careful to update the labels of objects in the image as well.\n",
        "- **ColorJitter**: Randomly changes the brightness, contrast, saturation, and hue of the image.\n",
        "- **Mosaic Augmentation**: Mosaic augmentation is a technique that combines four images into one by randomly selecting a center image and placing the other three images around it. This can help the model learn to detect objects in different contexts and improve its performance. Here as well we should be careful to update the labels of objects in the image."
      ],
      "metadata": {
        "id": "-MQ8nx5Fk0AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resizing is used to resize the images to a fixed size before feeding them to the model. This can help improve the performance of the model and reduce the computational resources required for training."
      ],
      "metadata": {
        "id": "w7UDafUnk0AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_aug():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.Blur(blur_limit=3, p=0.5),\n",
        "                    A.MotionBlur(blur_limit=3, p=0.5),\n",
        "                    A.MedianBlur(blur_limit=3, p=0.5),\n",
        "                ],\n",
        "                p=0.5,\n",
        "            ),\n",
        "            A.ToGray(p=0.1),\n",
        "            A.RandomBrightnessContrast(p=0.1),\n",
        "            A.ColorJitter(p=0.1),\n",
        "            A.RandomGamma(p=0.1),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(p=0.5),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(\n",
        "            format=\"pascal_voc\",\n",
        "            label_fields=[\"labels\"],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def get_train_transform():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            ToTensorV2(p=1.0),\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(\n",
        "            format=\"pascal_voc\",\n",
        "            label_fields=[\"labels\"],\n",
        "        ),\n",
        "    )"
      ],
      "metadata": {
        "id": "GevVKeOrk0AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_valid_transform():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            ToTensorV2(p=1.0),\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(\n",
        "            format=\"pascal_voc\",\n",
        "            label_fields=[\"labels\"],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def infer_transforms(image):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    )\n",
        "    return transform(image)"
      ],
      "metadata": {
        "id": "IjUszLdXk0AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_name, image_dir, label_dir):\n",
        "    image_path = os.path.sep.join([image_dir, image_name])\n",
        "    name = image_name.split(\".\")[0]\n",
        "    label_name = name + \".txt\"\n",
        "    label_path = os.path.sep.join([label_dir, label_name])\n",
        "\n",
        "    return image_path, label_path, label_name\n",
        "\n",
        "\n",
        "def draw_rect(img, bboxes, color=(255, 0, 0)):\n",
        "    img = img.copy()\n",
        "    height, width = img.shape[:2]\n",
        "    for bbox in bboxes:\n",
        "        center_x, center_y, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "        x = int((center_x - w / 2) * width)\n",
        "        w = int(w * width)\n",
        "        y = int((center_y - h / 2) * height)\n",
        "        h = int(h * height)\n",
        "        img = cv2.rectangle(img, (x, y), (x + w, y + h), color, 1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def read_img(image_path, cvt_color=True):\n",
        "    img = cv2.imread(image_path)\n",
        "    if cvt_color:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_img(image, save_path, jpg_quality=None):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    if jpg_quality:\n",
        "        cv2.imwrite(save_path, image, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n",
        "    else:\n",
        "        cv2.imwrite(save_path, image)\n",
        "\n",
        "\n",
        "def read_label(label_path):\n",
        "    with open(label_path) as f:\n",
        "        conts = f.readlines()\n",
        "\n",
        "    bboxes = []\n",
        "    class_labels = []\n",
        "    for cont in conts:\n",
        "        cont = cont.strip().split()\n",
        "        center_x, center_y, w, h = (\n",
        "            float(cont[1]),\n",
        "            float(cont[2]),\n",
        "            float(cont[3]),\n",
        "            float(cont[4]),\n",
        "        )\n",
        "        bboxes.append([center_x, center_y, w, h])\n",
        "        class_labels.append(cont[0])\n",
        "    return (bboxes, class_labels)\n",
        "\n",
        "\n",
        "def display_img(image_path, label_path):\n",
        "    img = read_img(image_path, cvt_color=False)\n",
        "    bboxes = read_label(label_path)[0]\n",
        "    img = draw_rect(img, bboxes)\n",
        "    cv2.imshow(\"Image\", img)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "\n",
        "def save_label(bboxes, class_labels, label_path):\n",
        "    tem_lst = []\n",
        "    for i, bbox in enumerate(bboxes):\n",
        "        tem_lst.append(\n",
        "            class_labels[i]\n",
        "            + \" \"\n",
        "            + str(bbox[0])\n",
        "            + \" \"\n",
        "            + str(bbox[1])\n",
        "            + \" \"\n",
        "            + str(bbox[2])\n",
        "            + \" \"\n",
        "            + str(bbox[3])\n",
        "            + \"\\n\"\n",
        "        )\n",
        "\n",
        "    with open(label_path, \"w\") as f:\n",
        "        f.writelines(tem_lst)"
      ],
      "metadata": {
        "id": "Ug3dvj-jk0AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_crop_savebboxes(\n",
        "    image_name, image_dir, label_dir, expected_h, expected_w, min_area, min_visibility\n",
        "):\n",
        "    image_path, label_path, _ = preprocess(image_name, image_dir, label_dir)\n",
        "\n",
        "    (bboxes, class_labels) = read_label(label_path)\n",
        "\n",
        "    transform = A.Compose(\n",
        "        [A.RandomResizedCrop(expected_h, expected_w)],\n",
        "        bbox_params=A.BboxParams(\n",
        "            format=\"yolo\",\n",
        "            label_fields=[\"class_labels\"],\n",
        "            min_area=min_area,\n",
        "            min_visibility=min_visibility,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    transformed = transform(image=read_img(image_path), bboxes=bboxes, class_labels=class_labels)\n",
        "    transformed_image = transformed[\"image\"]\n",
        "    transformed_bboxes = transformed[\"bboxes\"]\n",
        "    transformed_class_labels = transformed[\"class_labels\"]\n",
        "\n",
        "    return transformed_image, transformed_bboxes, transformed_class_labels"
      ],
      "metadata": {
        "id": "PMxjyEdMk0AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mosaic(\n",
        "    image_file_list,\n",
        "    image_dir,\n",
        "    label_dir,\n",
        "    output_image_dir,\n",
        "    output_label_dir,\n",
        "    mo_w,\n",
        "    mo_h,\n",
        "    scale_x,\n",
        "    scale_y,\n",
        "    min_area,\n",
        "    min_visibility,\n",
        "    show_image=False,\n",
        "):\n",
        "\n",
        "    new_img = np.zeros((mo_h, mo_w, 3), dtype=\"uint8\")\n",
        "\n",
        "    div_point_x = int(mo_w * scale_x)\n",
        "    div_point_y = int(mo_h * scale_y)\n",
        "\n",
        "    for i in range(len(image_file_list)):\n",
        "        if i == 0:\n",
        "            w0 = div_point_x\n",
        "            h0 = div_point_y\n",
        "            img_0, bboxes_0, class_labels_0 = random_crop_savebboxes(\n",
        "                image_file_list[0],\n",
        "                image_dir,\n",
        "                label_dir,\n",
        "                h0,\n",
        "                w0,\n",
        "                min_area,\n",
        "                min_visibility,\n",
        "            )\n",
        "            new_img[:div_point_y, :div_point_x, :] = img_0\n",
        "\n",
        "            if len(bboxes_0) == 0:\n",
        "                bboxes_0_new = []\n",
        "            else:\n",
        "                bboxes_0_new = np.zeros((len(bboxes_0), 4))\n",
        "                bboxes_0_new = bboxes_0_new.tolist()\n",
        "\n",
        "            for i, box in enumerate(bboxes_0):\n",
        "                bboxes_0_new[i][0] = box[0] * scale_x\n",
        "                bboxes_0_new[i][2] = box[2] * scale_x\n",
        "\n",
        "                bboxes_0_new[i][1] = box[1] * scale_y\n",
        "                bboxes_0_new[i][3] = box[3] * scale_y\n",
        "\n",
        "        elif i == 1:\n",
        "            w1 = mo_w - div_point_x\n",
        "            h1 = div_point_y\n",
        "            img_1, bboxes_1, class_labels_1 = random_crop_savebboxes(\n",
        "                image_file_list[1],\n",
        "                image_dir,\n",
        "                label_dir,\n",
        "                h1,\n",
        "                w1,\n",
        "                min_area,\n",
        "                min_visibility,\n",
        "            )\n",
        "            new_img[:div_point_y, div_point_x:, :] = img_1\n",
        "\n",
        "            if len(bboxes_1) == 0:\n",
        "                bboxes_1_new = []\n",
        "            else:\n",
        "                bboxes_1_new = np.zeros((len(bboxes_1), 4))\n",
        "                bboxes_1_new = bboxes_1_new.tolist()\n",
        "\n",
        "            for i, box in enumerate(bboxes_1):\n",
        "                bboxes_1_new[i][0] = box[0] * (1 - scale_x) + scale_x\n",
        "                bboxes_1_new[i][2] = box[2] * (1 - scale_x)\n",
        "\n",
        "                bboxes_1_new[i][1] = box[1] * scale_y\n",
        "                bboxes_1_new[i][3] = box[3] * scale_y\n",
        "\n",
        "        elif i == 2:\n",
        "            w2 = div_point_x\n",
        "            h2 = mo_h - div_point_y\n",
        "            img_2, bboxes_2, class_labels_2 = random_crop_savebboxes(\n",
        "                image_file_list[2],\n",
        "                image_dir,\n",
        "                label_dir,\n",
        "                h2,\n",
        "                w2,\n",
        "                min_area,\n",
        "                min_visibility,\n",
        "            )\n",
        "            new_img[div_point_y:, :div_point_x, :] = img_2\n",
        "\n",
        "            if len(bboxes_2) == 0:\n",
        "                bboxes_2_new = []\n",
        "            else:\n",
        "                bboxes_2_new = np.zeros((len(bboxes_2), 4))\n",
        "                bboxes_2_new = bboxes_2_new.tolist()\n",
        "\n",
        "            for i, box in enumerate(bboxes_2):\n",
        "                bboxes_2_new[i][0] = box[0] * scale_x\n",
        "                bboxes_2_new[i][2] = box[2] * scale_x\n",
        "\n",
        "                bboxes_2_new[i][1] = box[1] * (1 - scale_y) + scale_y\n",
        "                bboxes_2_new[i][3] = box[3] * (1 - scale_y)\n",
        "\n",
        "        else:\n",
        "            w3 = mo_w - div_point_x\n",
        "            h3 = mo_h - div_point_y\n",
        "            img_3, bboxes_3, class_labels_3 = random_crop_savebboxes(\n",
        "                image_file_list[3],\n",
        "                image_dir,\n",
        "                label_dir,\n",
        "                h3,\n",
        "                w3,\n",
        "                min_area,\n",
        "                min_visibility,\n",
        "            )\n",
        "            new_img[div_point_y:, div_point_x:, :] = img_3\n",
        "\n",
        "            if len(bboxes_3) == 0:\n",
        "                bboxes_3_new = []\n",
        "            else:\n",
        "                bboxes_3_new = np.zeros((len(bboxes_3), 4))\n",
        "                bboxes_3_new = bboxes_3_new.tolist()\n",
        "\n",
        "            for i, box in enumerate(bboxes_3):\n",
        "                bboxes_3_new[i][0] = box[0] * (1 - scale_x) + scale_x\n",
        "                bboxes_3_new[i][2] = box[2] * (1 - scale_x)\n",
        "\n",
        "                bboxes_3_new[i][1] = box[1] * (1 - scale_y) + scale_y\n",
        "                bboxes_3_new[i][3] = box[3] * (1 - scale_y)\n",
        "\n",
        "    new_class_labels = class_labels_0 + class_labels_1 + class_labels_2 + class_labels_3\n",
        "    new_bboxes = bboxes_0_new + bboxes_1_new + bboxes_2_new + bboxes_3_new\n",
        "\n",
        "    image_store_path = os.path.sep.join(\n",
        "        [\n",
        "            output_image_dir,\n",
        "            + image_file_list[0].split(\".\")[0]\n",
        "            + \"_\"\n",
        "            + image_file_list[1].split(\".\")[0]\n",
        "            + \"_\"\n",
        "            + image_file_list[2].split(\".\")[0]\n",
        "            + \"_\"\n",
        "            + image_file_list[3].split(\".\")[0]\n",
        "            + \".jpg\",\n",
        "        ]\n",
        "    )\n",
        "    label_store_path = os.path.sep.join(\n",
        "        [\n",
        "            output_label_dir,\n",
        "            + image_file_list[0].split(\".\")[0]\n",
        "            + \"_\"\n",
        "            + image_file_list[1].split(\".\")[0]\n",
        "            + \"_\"\n",
        "            + image_file_list[2].split(\".\")[0]\n",
        "            + \"_\"\n",
        "            + image_file_list[3].split(\".\")[0]\n",
        "            + \".txt\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    save_img(new_img, image_store_path)\n",
        "    save_label(new_bboxes, new_class_labels, label_store_path)\n",
        "\n",
        "    if show_image:\n",
        "        cv2.imshow(\"Mosaic Image\", new_img)\n",
        "        cv2.waitKey(0)"
      ],
      "metadata": {
        "id": "ubRunZd9k0AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_text(\n",
        "    img,\n",
        "    text,\n",
        "    font=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "    pos=(0, 0),\n",
        "    font_scale=1,\n",
        "    font_thickness=2,\n",
        "    text_color=(0, 255, 0),\n",
        "    text_color_bg=(0, 0, 0),\n",
        "):\n",
        "    offset = (5, 5)\n",
        "    x, y = pos\n",
        "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
        "    text_w, text_h = text_size\n",
        "    rec_start = tuple(x - y for x, y in zip(pos, offset))\n",
        "    rec_end = tuple(x + y for x, y in zip((x + text_w, y + text_h), offset))\n",
        "    cv2.rectangle(img, rec_start, rec_end, text_color_bg, -1)\n",
        "    cv2.putText(\n",
        "        img,\n",
        "        text,\n",
        "        (x, int(y + text_h + font_scale - 1)),\n",
        "        font,\n",
        "        font_scale,\n",
        "        text_color,\n",
        "        font_thickness,\n",
        "        cv2.LINE_AA,\n",
        "    )\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "1Xc4zgH4k0Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mosaic(\n",
        "    [\"274.jpg\", \"275.jpg\", \"276.jpg\", \"280.jpg\"],\n",
        "    TRAIN_IMAGES,\n",
        "    TRAIN_LABELS,\n",
        "    TRAIN_DESTINATION,\n",
        "    TRAIN_LABELS,\n",
        "    640,\n",
        "    640,\n",
        "    0.5,\n",
        "    0.5,\n",
        "    0.1,\n",
        "    0.3,\n",
        "    show_image=True,\n",
        ")"
      ],
      "metadata": {
        "id": "6edmB2fSk0Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader\n",
        "\n",
        "We will create a custom dataset class to load the images and annotations from the dataset and apply the data augmentation techniques. We will also create a data loader to load the data in batches and shuffle it during training."
      ],
      "metadata": {
        "id": "LGJzTosjk0Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_path,\n",
        "        labels_path,\n",
        "        img_size,\n",
        "        classes,\n",
        "        transforms=None,\n",
        "        use_train_aug=False,\n",
        "        train=False,\n",
        "        mosaic=1.0,\n",
        "        square_training=False,\n",
        "    ):\n",
        "        self.transforms = transforms\n",
        "        self.use_train_aug = use_train_aug\n",
        "        self.images_path = images_path\n",
        "        self.labels_path = labels_path\n",
        "        self.img_size = img_size\n",
        "        self.classes = classes\n",
        "        self.train = train\n",
        "        self.square_training = square_training\n",
        "        self.mosaic_border = [-img_size // 2, -img_size // 2]\n",
        "        self.all_image_paths = []\n",
        "        self.mosaic = mosaic\n",
        "\n",
        "        for file_type in self.image_file_types:\n",
        "            self.all_image_paths.extend(\n",
        "                glob.glob(os.path.join(self.images_path, file_type))\n",
        "            )\n",
        "        self.all_images = [\n",
        "            image_path.split(os.path.sep)[-1] for image_path in [\"*.jpg\", \"*.jpeg\"]\n",
        "        ]\n",
        "        self.all_images = sorted(self.all_images)\n",
        "\n",
        "        self.all_labels = [\n",
        "            label_path.split(os.path.sep)[-1] for label_path in glob.glob(os.path.join(self.labels_path, \"*.txt\"))\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.all_images[idx]\n",
        "        label = self.all_labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    To handle the data loading as different images may have different number\n",
        "    of objects and to handle varying size tensors as well.\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "3PLgqkodk0Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_dataset(\n",
        "    train_dir_images,\n",
        "    train_dir_labels,\n",
        "    img_size,\n",
        "    classes,\n",
        "    use_train_aug=False,\n",
        "    mosaic=1.0,\n",
        "    square_training=False,\n",
        "):\n",
        "    train_dataset = CustomDataset(\n",
        "        train_dir_images,\n",
        "        train_dir_labels,\n",
        "        img_size,\n",
        "        classes,\n",
        "        get_train_transform(),\n",
        "        use_train_aug=use_train_aug,\n",
        "        train=True,\n",
        "        mosaic=mosaic,\n",
        "        square_training=square_training,\n",
        "    )\n",
        "    return train_dataset\n",
        "\n",
        "\n",
        "def create_valid_dataset(valid_dir_images, valid_dir_labels, img_size, classes, square_training=False):\n",
        "    valid_dataset = CustomDataset(\n",
        "        valid_dir_images,\n",
        "        valid_dir_labels,\n",
        "        img_size,\n",
        "        classes,\n",
        "        get_valid_transform(),\n",
        "        train=False,\n",
        "        square_training=square_training,\n",
        "    )\n",
        "    return valid_dataset\n",
        "\n",
        "\n",
        "def create_train_loader(train_dataset, batch_size, num_workers=0, batch_sampler=None):\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        sampler=batch_sampler,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def create_valid_loader(valid_dataset, batch_size, num_workers=0, batch_sampler=None):\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        sampler=batch_sampler,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "    return valid_loader"
      ],
      "metadata": {
        "id": "RXmR8Anpk0Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_train_dataset(\n",
        "    TRAIN_IMAGES,\n",
        "    TRAIN_LABELS,\n",
        "    (640, 640),\n",
        "    classes,\n",
        "    use_train_aug=True,\n",
        "    mosaic=1.0,\n",
        ")\n",
        "\n",
        "valid_dataset = create_valid_dataset(\n",
        "    VAL_IMAGES,\n",
        "    VAL_LABELS,\n",
        "    (640, 640),\n",
        "    classes,\n",
        ")\n",
        "\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "valid_sampler = SequentialSampler(valid_dataset)\n",
        "\n",
        "train_loader = create_train_loader(train_dataset, batch_size=2, num_workers=2, batch_sampler=train_sampler)\n",
        "valid_loader = create_valid_loader(valid_dataset, batch_size=2, num_workers=2, batch_sampler=valid_sampler)"
      ],
      "metadata": {
        "id": "7t3ierFsk0Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster R-CNN Model\n",
        "\n",
        "In this step, we will create the Faster R-CNN model using the PyTorch library. We will use a pre-trained ResNet-101 backbone and replace the classification and regression heads with new heads for our dataset. We will also define the loss function and optimizer for training the model.\n",
        "\n",
        "RCNN model is defined with the following components:\n",
        "- **Backbone**: A pre-trained ResNet-101 backbone is used to extract features from the input images.\n",
        "- **Region Proposal Network (RPN)**: A region proposal network is used to generate region proposals for objects in the images.\n",
        "- **RoI Pooling**: RoI pooling is used to extract features from the region proposals and resize them to a fixed size.\n",
        "- **Head**: The head of the model consists of two subnetworks: a classification subnetwork that predicts the class of the object in the region proposal and a regression subnetwork that predicts the bounding box coordinates of the object.\n",
        "- **Loss Function**: The loss function used to train the model is a combination of the classification and regression losses.\n",
        "- **Optimizer**: The SGD optimizer is used to optimize the model parameters during training.\n",
        "- **Learning Rate Scheduler**: A learning rate scheduler is used to adjust the learning rate during training.\n",
        "\n",
        "The important difference here is that we are using GIOU instead of the default IOU loss. GIOU is a loss function used to measure the similarity between two bounding boxes. This loss function can help the model learn to predict more accurate bounding boxes."
      ],
      "metadata": {
        "id": "9Mgi5bfOk0Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GIOU(boxes1, boxes2):\n",
        "    EPSILON = 1e-6\n",
        "    boxes1 = torch.cat((boxes1[..., :2] - boxes1[..., 2:] / 2, boxes1[..., :2] + boxes1[..., 2:] / 2), 1)\n",
        "    boxes2 = torch.cat((boxes2[..., :2] - boxes2[..., 2:] / 2, boxes2[..., :2] + boxes2[..., 2:] / 2), 1)\n",
        "\n",
        "    inter_left_up = torch.max(boxes1[..., :2], boxes2[..., :2])\n",
        "    inter_right_down = torch.min(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = torch.max(inter_right_down - inter_left_up, torch.tensor(0.0).to(device))\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "\n",
        "    area1 = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    area2 = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "\n",
        "    union_area = area1 + area2 - inter_area\n",
        "\n",
        "    iou = inter_area / (union_area + EPSILON)\n",
        "\n",
        "    enclose_left_up = torch.min(boxes1[..., :2], boxes2[..., :2])\n",
        "    enclose_right_down = torch.max(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    enclose_section = torch.max(enclose_right_down - enclose_left_up, torch.tensor(0.0).to(device))\n",
        "    enclose_area = enclose_section[..., 0] * enclose_section[..., 1]\n",
        "\n",
        "    giou = iou - 1.0 * (enclose_area - union_area) / (enclose_area + EPSILON)\n",
        "\n",
        "    return giou"
      ],
      "metadata": {
        "id": "o1WtH1-ak0Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=7):\n",
        "    model_backbone = torchvision.models.resnet101(weights=\"DEFAULT\")\n",
        "\n",
        "    conv1 = model_backbone.conv1\n",
        "    bn1 = model_backbone.bn1\n",
        "    relu = model_backbone.relu\n",
        "    max_pool = model_backbone.maxpool\n",
        "    layer1 = model_backbone.layer1\n",
        "    layer2 = model_backbone.layer2\n",
        "    layer3 = model_backbone.layer3\n",
        "    layer4 = model_backbone.layer4\n",
        "\n",
        "    backbone = nn.Sequential(conv1, bn1, relu, max_pool, layer1, layer2, layer3, layer4)\n",
        "    backbone.out_channels = 2048\n",
        "\n",
        "    # Here, we are using 5x3 anchors.\n",
        "    # Meaning, anchors with 5 different sizes and 3 different aspect ratios.\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )\n",
        "\n",
        "    roi_pooler = ops.MultiScaleRoIAlign(\n",
        "        featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    model = FasterRCNN(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        rpn_pre_nms_top_n_train=2000,\n",
        "        rpn_pre_nms_top_n_test=1000,\n",
        "        rpn_post_nms_top_n_train=2000,\n",
        "        rpn_post_nms_top_n_test=1000,\n",
        "        rpn_nms_thresh=0.7,\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "DOpNp_NMk0Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(num_classes=7)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)"
      ],
      "metadata": {
        "id": "19-twQXak0Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_box_loss = []\n",
        "train_class_loss = []\n",
        "valid_precision = []\n",
        "valid_recall = []\n",
        "valid_mAP50 = []\n",
        "valid_mAP50_95 = []\n",
        "valid_box_loss = []\n",
        "valid_class_loss = []\n",
        "\n",
        "for epoch in range(10):\n",
        "\n",
        "    model.train()\n",
        "    for images, targets in train_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        predicted_boxes = loss_dict[\"boxes\"]\n",
        "        target_boxes = [t[\"boxes\"] for t in targets]\n",
        "        giou_loss = GIOU(predicted_boxes, target_boxes)\n",
        "        losses += giou_loss\n",
        "\n",
        "        train_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
        "        train_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    for images, targets in valid_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        predicted_boxes = loss_dict[\"boxes\"]\n",
        "        target_boxes = [t[\"boxes\"] for t in targets]\n",
        "        giou_loss = GIOU(predicted_boxes, target_boxes)\n",
        "        losses += giou_loss\n",
        "\n",
        "        valid_box_loss.append(loss_dict[\"loss_box_reg\"].item())\n",
        "        valid_class_loss.append(loss_dict[\"loss_classifier\"].item())\n",
        "\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        precision = []\n",
        "        recall = []\n",
        "        AP50 = []\n",
        "        AP50_95 = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "\n",
        "        for i in range(len(targets)):\n",
        "            target = targets[i]\n",
        "            output = outputs[i]\n",
        "\n",
        "            pred_boxes = output[\"boxes\"]\n",
        "            pred_scores = output[\"scores\"]\n",
        "            pred_labels = output[\"labels\"]\n",
        "\n",
        "            true_boxes = target[\"boxes\"]\n",
        "            true_labels = target[\"labels\"]\n",
        "\n",
        "            for label in range(7):\n",
        "                true_boxes_label = true_boxes[true_labels == label]\n",
        "                pred_boxes_label = pred_boxes[pred_labels == label]\n",
        "                pred_scores_label = pred_scores[pred_labels == label]\n",
        "\n",
        "                if len(true_boxes_label) == 0:\n",
        "                    precision.append(0)\n",
        "                    recall.append(0)\n",
        "                    AP50.append(0)\n",
        "                    AP50_95.append(0)\n",
        "                    continue\n",
        "\n",
        "                if len(pred_boxes_label) == 0:\n",
        "                    precision.append(0)\n",
        "                    recall.append(0)\n",
        "                    AP50.append(0)\n",
        "                    AP50_95.append(0)\n",
        "                    continue\n",
        "\n",
        "                iou = GIOU(true_boxes_label, pred_boxes_label)\n",
        "                iou_thresholds = torch.linspace(0.5, 0.95, 10).to(device)\n",
        "\n",
        "                true_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
        "                false_positives = torch.zeros(len(iou_thresholds)).to(device)\n",
        "                false_negatives = torch.zeros(len(iou_thresholds)).to(device)\n",
        "\n",
        "                for i, iou_threshold in enumerate(iou_thresholds):\n",
        "                    true_positives[i] = torch.sum(iou > iou_threshold)\n",
        "                    false_positives[i] = torch.sum(iou <= iou_threshold)\n",
        "                    false_negatives[i] = len(true_boxes_label) - true_positives[i]\n",
        "\n",
        "                precision.append(true_positives / (true_positives + false_positives))\n",
        "                recall.append(true_positives / (true_positives + false_negatives))\n",
        "\n",
        "                AP50.append((precision[-1] * recall[-1]).mean())\n",
        "                AP50_95.append((precision[-1] * recall[-1]).mean())\n",
        "\n",
        "        valid_precision.append(precision[-1])\n",
        "        valid_recall.append(recall[-1])\n",
        "        valid_mAP50.append(AP50[-1])\n",
        "        valid_mAP50_95.append(AP50_95[-1])"
      ],
      "metadata": {
        "id": "KEGAP_CWk0Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 15))\n",
        "\n",
        "plt.subplot(4, 2, 1)\n",
        "plt.plot(train_box_loss)\n",
        "plt.title(\"Train Box Loss\")\n",
        "\n",
        "plt.subplot(4, 2, 2)\n",
        "plt.plot(train_class_loss)\n",
        "plt.title(\"Train Class Loss\")\n",
        "\n",
        "plt.subplot(4, 2, 3)\n",
        "plt.plot(valid_precision)\n",
        "plt.title(\"Validation Precision\")\n",
        "\n",
        "plt.subplot(4, 2, 4)\n",
        "plt.plot(valid_recall)\n",
        "plt.title(\"Validation Recall\")\n",
        "\n",
        "plt.subplot(4, 2, 5)\n",
        "plt.plot(valid_mAP50)\n",
        "plt.title(\"Validation mAP50\")\n",
        "\n",
        "plt.subplot(4, 2, 6)\n",
        "plt.plot(valid_mAP50_95)\n",
        "plt.title(\"Validation mAP50-95\")\n",
        "\n",
        "plt.subplot(4, 2, 7)\n",
        "plt.plot(valid_box_loss)\n",
        "plt.title(\"Validation Box Loss\")\n",
        "\n",
        "plt.subplot(4, 2, 8)\n",
        "plt.plot(valid_class_loss)\n",
        "plt.title(\"Validation Class Loss\")\n",
        "\n",
        "plt.suptitle(\"Training Metrics and Loss\", fontsize=24)\n",
        "plt.subplots_adjust(top=0.8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A9LRxBCXk0Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = create_valid_dataset(\n",
        "    TEST_IMAGES,\n",
        "    TEST_LABELS,\n",
        "    (640, 640),\n",
        "    classes,\n",
        "    batch_size=1,\n",
        ")\n",
        "\n",
        "test_loader = create_valid_loader(test_dataset, batch_size=1, num_workers=2)"
      ],
      "metadata": {
        "id": "GXD-jkUkk0Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = 0\n",
        "for image, target in test_loader:\n",
        "    image = list(image.to(device) for image in image)\n",
        "    target = [{k: v.to(device) for k, v in t.items()} for t in target]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "\n",
        "    class_ids = output[0][\"labels\"]\n",
        "    boxes = output[0][\"boxes\"]\n",
        "    scores = output[0][\"scores\"]\n",
        "\n",
        "    axs = plt.subplots(1, 2, figsize=(15, 15))\n",
        "\n",
        "    original_image = np.copy(cv2.imread(os.path.join(TEST_IMAGES, image[0])))\n",
        "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    original_labels = open(os.path.join(TEST_LABELS, target[0][\"labels\"][0])).readlines()\n",
        "    original_boxes = [list(map(float, label.split()[1:])) for label in original_labels]\n",
        "\n",
        "    for box in original_boxes:\n",
        "        h, w, _ = original_image.shape\n",
        "        x_min = int((box[0] - box[2] / 2) * w)\n",
        "        y_min = int((box[1] - box[3] / 2) * h)\n",
        "        x_max = int((box[0] + box[2] / 2) * w)\n",
        "        y_max = int((box[1] + box[3] / 2) * h)\n",
        "\n",
        "        cv2.rectangle(original_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "        cv2.putText(\n",
        "            original_image,\n",
        "            Idx2Label[int(box[0])],\n",
        "            (x_min, y_min),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            fontScale=1,\n",
        "            color=(255, 255, 255),\n",
        "            thickness=2,\n",
        "        )\n",
        "\n",
        "    detected_image = np.copy(original_image)\n",
        "    detected_image = cv2.cvtColor(detected_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i]\n",
        "        class_id = class_ids[i]\n",
        "        score = scores[i]\n",
        "\n",
        "        h, w, _ = original_image.shape\n",
        "        x_min = int((box[0] - box[2] / 2) * w)\n",
        "        y_min = int((box[1] - box[3] / 2) * h)\n",
        "        x_max = int((box[0] + box[2] / 2) * w)\n",
        "        y_max = int((box[1] + box[3] / 2) * h)\n",
        "\n",
        "        cv2.rectangle(detected_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "        cv2.putText(\n",
        "            detected_image,\n",
        "            Idx2Label[int(class_id)] + \" \" + str(score),\n",
        "            (x_min, y_min),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            fontScale=1,\n",
        "            color=(255, 255, 255),\n",
        "            thickness=2,\n",
        "        )\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Real Bounding Boxes\")\n",
        "    plt.imshow(original_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Predicted Bounding Boxes\")\n",
        "    plt.imshow(detected_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    num += 1\n",
        "\n",
        "    if num == 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "YJTZTaRpk0Ak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}